# tests/test_e2e_flow.py
from __future__ import annotations
import json
from pathlib import Path

import numpy as np
from PIL import Image

from neunet.config import default_config, save_config, resolve_config
from neunet.train import train as train_fn
from neunet.eval import evaluate as eval_fn
from neunet.report_train import generate_train_log_report
from neunet.infer import infer as infer_fn
from neunet.report import build_report

def _make_config(tmp: Path) -> tuple[dict, Path]:
    cfg = default_config()
    cfg["seed"] = 7
    cfg["data"]["root"] = str(tmp / "data")
    cfg["data"]["num_workers"] = 0
    cfg["data"]["val_split"] = 0.2
    cfg["train"]["epochs"] = 2
    cfg["train"]["batch_size"] = 16
    cfg["train"]["lr"] = 1e-3
    cfg["train"]["weight_decay"] = 1e-4
    cfg["model"]["layers"] = [64]
    cfg["model"]["activations"] = ["relu"]
    cfg["model"]["dropout"] = 0.1
    cfg["artifacts"]["models_dir"] = str(tmp / "models")
    cfg["artifacts"]["runs_dir"] = str(tmp / "runs")

    cfg_path = tmp / "configs" / "default.yaml"
    cfg_path.parent.mkdir(parents=True, exist_ok=True)
    save_config(cfg, cfg_path)
    return resolve_config(cfg), cfg_path

def _make_dummy_images(root: Path):
    """
    Create a few 28x28 grayscale PNGs under folders named with their label,
    so the inference report can extract a true label.
    """
    for label in (0, 3, 7):
        d = root / str(label)
        d.mkdir(parents=True, exist_ok=True)
        img = (np.random.rand(28, 28) * 255).astype("uint8")
        Image.fromarray(img, mode="L").save(d / f"sample_{label}.png")

def test_basic_flow(tmp_path):
    tmp = Path(tmp_path)

    # 1) config
    cfg, cfg_path = _make_config(tmp)

    # 2) train (uses patched TinyMNIST)
    train_fn(cfg)
    models = tmp / "models"
    runs = tmp / "runs"
    assert (models / "best.pt").exists()
    assert (runs / "metrics.json").exists()
    # train log should be generated by train.py if integrated; if not, create below
    # assert (runs / "train_log.md").exists()

    # 3) eval (test set)
    res_eval = eval_fn(cfg, checkpoint=str(models / "best.pt"),
                       metrics_out=str(runs / "metrics.json"),
                       plots_out=str(runs))
    assert "test_acc" in res_eval and isinstance(res_eval["test_acc"], float)
    assert (runs / "confusion_matrix.png").exists() or (runs / "confusion_matrix.json").exists()

    # 4) training log (explicitly generate to ensure file exists)
    out_log = runs / "train_log.md"
    _ = generate_train_log_report(metrics_path=str(runs / "metrics.json"),
                                  eval_path=None,
                                  out_path=str(out_log))
    assert out_log.exists()
    assert "Training Log" in out_log.read_text(encoding="utf-8")

    # 5) inference on custom images
    imgs_dir = tmp / "my_imgs2"
    _make_dummy_images(imgs_dir)
    infer_out = runs / "infer.json"
    _ = infer_fn(str(imgs_dir), checkpoint=str(models / "best.pt"),
                 out=str(infer_out), topk=3, recursive=True, config=str(cfg_path))
    assert infer_out.exists()
    with infer_out.open("r", encoding="utf-8") as f:
        items = json.load(f)
    assert isinstance(items, list) and len(items) >= 3
    assert set(items[0].keys()) >= {"file", "pred"}

    # 6) inference report
    report_md = runs / "inference_report.md"
    build_report(str(infer_out), str(report_md))
    assert report_md.exists()
    txt = report_md.read_text(encoding="utf-8")
    assert "Inference Report" in txt and "Accuracy" in txt
